{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tiff\n",
    "import cv2 as cv\n",
    "import albumentations as albu\n",
    "\n",
    "import pathlib, sys, os, random, time\n",
    "import numba, gc, cv2\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as D\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, OneOf, Normalize, HorizontalFlip, VerticalFlip, RandomRotate90, Transpose, ShiftScaleRotate, IAAAdditiveGaussianNoise, IAAPerspective,\n",
    "    CLAHE, RandomBrightness, RandomGamma, IAASharpen, Blur, MotionBlur, RandomContrast, HueSaturationValue, VerticalFlip,\n",
    "    RandomRotate90, OneOf, Resize, Rotate, RandomBrightnessContrast, Lambda\n",
    "    )\n",
    "from albumentations.pytorch import ToTensorV2, ToTensor\n",
    "#from albumentations import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_DIR = '/home/arti/DL/HuBMAP/Data/'\n",
    "MODEL_SAVE_DIR = \"/home/arti/DL/HuBMAP/OUTPUTS/models/\"\n",
    "TILE_SIZE = 256\n",
    "REDUCE_RATE = 4\n",
    "SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "WINDOW=1024\n",
    "MIN_OVERLAP=32\n",
    "NEW_SIZE=256\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(shape, window=256, min_overlap=32):\n",
    "    \"\"\"\n",
    "        Return Array of size (N,4), where N - number of tiles,\n",
    "        2nd axis represente slices: x1,x2,y1,y2 \n",
    "    \"\"\"\n",
    "    x, y = shape\n",
    "    nx = x // (window - min_overlap) + 1\n",
    "    x1 = np.linspace(0, x, num=nx, endpoint=False, dtype=np.int64)\n",
    "    x1[-1] = x - window\n",
    "    x2 = (x1 + window).clip(0, x)\n",
    "    ny = y // (window - min_overlap) + 1\n",
    "    y1 = np.linspace(0, y, num=ny, endpoint=False, dtype=np.int64)\n",
    "    y1[-1] = y - window\n",
    "    y2 = (y1 + window).clip(0, y)\n",
    "    slices = np.zeros((nx,ny, 4), dtype=np.int64)\n",
    "    \n",
    "    for i in range(nx):\n",
    "        for j in range(ny):\n",
    "            slices[i,j] = x1[i], x2[i], y1[j], y2[j]    \n",
    "    return slices.reshape(nx*ny,4)\n",
    "\n",
    "@numba.njit()\n",
    "def rle_numba(pixels):\n",
    "    size = len(pixels)\n",
    "    points = []\n",
    "    if pixels[0] == 1: points.append(0)\n",
    "    flag = True\n",
    "    for i in range(1, size):\n",
    "        if pixels[i] != pixels[i-1]:\n",
    "            if flag:\n",
    "                points.append(i+1)\n",
    "                flag = False\n",
    "            else:\n",
    "                points.append(i+1 - points[-1])\n",
    "                flag = True\n",
    "    if pixels[-1] == 1: points.append(size-points[-1]+1)    \n",
    "    return points\n",
    "\n",
    "def rle_numba_encode(image):\n",
    "    pixels = image.flatten(order = 'F')\n",
    "    points = rle_numba(pixels)\n",
    "    return ' '.join(str(x) for x in points)\n",
    "\n",
    "def rle_decode(mask_rle, shape=(256, 256)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape, order='F')\n",
    "\n",
    "def visualize_tensor(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# ValTransforms\n",
    "# ====================================================\n",
    "def ValTransforms():\n",
    "    return Compose([\n",
    "        Resize(256,256),\n",
    "        Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        ),\n",
    "            #ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(encoder_name='se_resnext50_32x4d', \n",
    "                 encoder_weights='imagenet', \n",
    "                 activation='sigmoid')\n",
    "\n",
    "model = torch.load(os.path.join(MODEL_SAVE_DIR, 'best_model_sub.pth'), map_location=torch.device(device))\n",
    "model.to(device);\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;35m0486052bb.tiff\u001b[0m            Predict.ipynb   Train.ipynb\r\n",
      "Mask_Encode_Decode.ipynb  submission.csv  Unet_HuBMAP.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arti/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:2964: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n",
      "  \"Using lambda is incompatible with multiprocessing. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5708fbeb9c44888a71bcdfe7dccd5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arti/anaconda3/lib/python3.7/site-packages/rasterio/__init__.py:207: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Predicting 0486052bb\n",
      "(25784, 34937)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "identity_trfm = Lambda(image = lambda x,cols=None,rows=None : x)\n",
    "identity = rasterio.Affine(1, 0, 0, 0, 1, 0)\n",
    "\n",
    "transforms = ValTransforms()\n",
    "\n",
    "p = pathlib.Path('/home/arti/DL/BasicCode/Segmentations/')\n",
    "subm = {}\n",
    "\n",
    "for i, filename in tqdm(enumerate(p.glob('*.tiff')), \n",
    "                        total = len(list(p.glob('*.tiff')))):\n",
    "    \n",
    "    print(f'{i+1} Predicting {filename.stem}')\n",
    "    \n",
    "    dataset = rasterio.open(filename.as_posix(), transform = identity)\n",
    "    print(dataset.shape)\n",
    "    slices = make_grid(dataset.shape, window=WINDOW, min_overlap=MIN_OVERLAP)\n",
    "    preds = np.zeros(dataset.shape, dtype=np.uint8)\n",
    "    \n",
    "    for (x1,x2,y1,y2) in slices:\n",
    "        image = dataset.read([1,2,3],\n",
    "                    window=Window.from_slices((x1,x2),(y1,y2))) \n",
    "        \n",
    "        # SLICE TO TORCH.TENSOR\n",
    "        image = np.moveaxis(image, 0, -1)  \n",
    "        # resize and normalize\n",
    "        image = transforms(image=image)['image']\n",
    "        # equivalent ToTensorV2\n",
    "        image = np.moveaxis(image, -1, 0)\n",
    "        image = torch.from_numpy(image)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # (3,256,256) -> (1,3,256,256)\n",
    "            image = image.float().to(device)[None]\n",
    "            \n",
    "            score = model(image)\n",
    "            score = score.cpu().numpy()[0][0]   \n",
    "                    \n",
    "            # UPscaling 256->1024\n",
    "            score = cv2.resize(score, (WINDOW, WINDOW)) \n",
    "            \n",
    "            #print(np.mean(score), np.median(score))\n",
    "            \n",
    "        \n",
    "        preds[x1:x2,y1:y2] = (score > 0.5).astype(np.uint8)      \n",
    "    subm[i] = {'id':filename.stem, 'predicted': rle_numba_encode(preds)}\n",
    "    del preds\n",
    "    gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0486052bb</td>\n",
       "      <td>101727681 20 101753453 37 101779234 43 1018050...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                          predicted\n",
       "0  0486052bb  101727681 20 101753453 37 101779234 43 1018050..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame.from_dict(subm, orient='index')\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAExCAYAAABxpKVSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAOCklEQVR4nO3dz6uk2VkH8OcEmUwycRJGguaqzMZZKAq9MQ1uXIzQaAjJ0l9EF24UFdwJwRAFIQtBBRX/giBRENy1OEvBWdko4kqdONghkYzDhGgi6nFxb3VqirpVb1W9b73nnOfzgYaZrrq336pb93zP85zzvm+ptQYAeb1n7QMAYF2CACA5QQCQnCAASE4QACQnCACSEwQAyQkCuldKeaOU8mNrHwf0ShAAJCcIGEYp5edLKX9dSvndUsrbpZR/LqX8yN3fv1lK+Uop5ee2nv+xUsrfllLeuXv8szvf71OllC+WUr5aSvmN7cqjlPKeUsqvl1L+6e7xL5RSXrryS4ZZCAJG8zAi/i4iviMiPh8RfxIRPxwR3xcRPxsRf1BK+cDdc78eEZ+KiA9FxMci4hdLKZ+MiCil/EBE/FFE/ExEfCQiPhgR37317/xqRHwyIn40Im4i4j8i4g+XfGGwlOJaQ/SulPJGRPxCRHxPRHy61vrK3d//UNyGwnfVWr9893dfjYhXa61P9nyf34uIWmv9tVLKZyLi+2utP3X32Psj4u2I+Ila61+VUv4xIn651vra3eMfiYh/jYj31Vr/Z9lXDPP6trUPAGb25a3//q+IiE0IbP3dByIiSikPI+JzEfGDEfFcRLw3Iv707nk3EfHm5otqrf95FyIbL0fEn5dS/m/r7/43Ir4zIv5tllcCV6I1RGafj4i/iIjvrbV+MCL+OCLK3WNfitsKIyIiSinvi9t208abEfHjtdYPbf15vtYqBOiOICCzb4+It2qt3yilfDQifnrrsT+LiI/fLTY/FxG/Gd8KiYjb0PjtUsrLERGllA+XUj5xrQOHOQkCMvuliPitUsrXIuIzEfGFzQO11n+IiF+J28XmL0XE1yLiKxHxzbun/H7cVhN/eff1fxO3C9XQHYvFMMHdTqO3I+KVWuu/rH08MCcVAdyjlPLxUsr7SykvRMTvRMTfR8Qb6x4VzE8QwP0+ERFP7/68EhE/WZXQDEhrCCA5FQFAcoIAILmTzix+rry3Ph8vLHUsACzkG/H1+O/6zbLvsZOC4Pl4IR6WV+c5KgCu5vXby2LtpTUEkJwgAEhOEAAkJwgAkhMEAMkJAoDkBAFAcoIAIDlBAJCcIABIThAAJCcIAJITBADJCQKA5AQBQHKCACA5QQCQnCAASO6kW1UyhsdPn0RExKObBysfyXGbY43o43ihRyqCRLYH1X3/36LN4N9qCDx++uTZH+hVqbVOfvKL5aXq5vX92R2kWh1Ue7Nv8Pfe0qrX62vxTn2r7HtMayiZKQNV1nbMqYH56OZB2veKsWgNJbIZqE5pY6zZ8thtuRxrwazRonl08+DZn95oabEhCBLYHqg2v/hTB4C1BrhT2y67gQFMZ40gmZ52DJ1Ci4Zr6vHzZo2AZ3r50J6qt9f1+OmT7o6ZcWkNJaZHvJ6RQ6C1z9QSW3xH+/kJgqSu0VMXNG1a8udy6hpUz3rdJLCP1tBAzu1bjvJhvtQ57Zo11lym/pyPPW/u9lSGwX9UKoKkDP7vds621DV2Kk39d6Y8b+7PQCufqd1WUOtnp7dARdCRuWdwS/9ijPCLN/cAP+dukxbf37WP6b6f19rH1TrbRzvR43a1llzSwjn0tee2ky75GU75+qU/L61uQ3Y5lfvZPtq50Xqv1x5ELn3/Dh3nOa9hiUpAG+TW5rIfPaz1tMQaQeNOmeG0tkunlb76RtZf8iW19HnbODcEdv87E0HQuO0P9dQdIi1o6XhG2ubXmqXf12tPbrJ+TgRBBy4ZyNa6Xv59xzs12DjNtcPuGv/etT6zU9tpLU1u5maNYDDbH+a1P7hTwoB2tb5BYc6+/tQQGHUtQUUwiH0ztNFn32sHXRZrfnbuqzz09eclCAanP84l1g6BVow+qXIeASTX65VQR23TLOXQeQQqAkis54vEqXbnY7GYoY0ya2z1dbS+oMw0KgKG1eMsd5+lLxkdMc8gPsr7nZGKAJLYHfh7vPpoq5VR7wQBwzr3ujOjWuL9GOm9zdzmEgQMbYRfaAP38rK3tQQB0A1BtgyLxQMaaXbT2hVVGdPoJ4wdIwgG0/O+8ENGez1c19Rbd2YMgQitoeH1vMsi8+LdMb29N2se7/bkqIf3ag2CYDAtXX30Unb9wHVoDdG0zCGwG+T71ksyvz/brCVdRkWQgMFiPL39TNes7np7r9bg6qN0IXuLqLc1Adrj6qMMQekvBFiG1hB0QACwJBUBZ7v2DN1gCMtQESR2Sd9ZCMA4VARJXTqQG5hhHCqCpOYYyIUBjEFFQBOcEATrURE0YKnrAd23BtDannQBAOtSEbA4s31om4qgAXPOyqecgbtWFXDfsbVQlUBmKoKB3Hcvgn0D7WaWfmimbiYPOagIBnbfTPuUwd3uIhifIFhBS4u1h/79zWM939wGOE4QXNmx2fglg+7Urznle2sNwfisEVzo1D76oUHYoAusQRDM5Jww2A2FFlsvmxt6t3hswDy0hmZy6kB53/MNuMC1CYILGbiB3mkNASQnCACSEwQAyQkCgOQEwQH29QMZCIJ7CAEgC0Fwj9G2hU652iiQkyA4YJQw2B38hQGwTRAAJCcIFtBaC6aHaxoB63GJiZltB8CU20ZeSyvHAbRHRQCQnCCY2fbMu4VZ+L42lR1EwDatoQW0EAARdgcB0wiCgT26ebA3DFoJKqANgmBwBn3gGEGwgO1ZuIEYaJ3F4oXp0wOtEwQz2x34VQRA6wTBzFrbPgpwjDWCBQgAoCcqAoDkBAFAcsO3hmzlBDhs+CBYmyACWqc1BJDcsBXBoRO5rnmfAFUA0Lohg+BQO8aZvgDvpjUEkNzQQbCvLaNVA/BuQ7aGjg32PYTBpoXVw7ECfRsyCKZoeVvn9rEdW9hu+XUAfRi6NZSNhXDgHIKgQY9uHjyb3Y/Q5gLaVmqtk5/8YnmpPiyvLng4XMM1z6MA2vB6fS3eqW+VfY+pCJLRPgJ2CYITjTKQjvI6gMsJghNsBs/HT590O5BOXXsA8hAEV9JScAgBYJsgOME59yPerh56riSAcQmCE21v7Zz6/EP/T9sENxkIgisSAv3xMyODtJeYONUl1/4xmAAtUxEAJKcimMisHhiVigAgOUEAkJwgAEhOEAAkZ7G4QbsnMc29UO0y1MA2FcECNpeSOOes1KXPZN2+3AVAhCBYxLF7DJ8SEpfO3F3fCDhGECxodxDfHZAPDdCnXtNon2PfHyDCrSqvbntwPhYUBmtgLoduVWmx+MoM7kBrtIYasn33MIEBXIsgaIwAAK5NEAAkZ41gokOLvAA9UxFMZPAHRqUiOMF2GFxyxzKAlqgIzrDdJnLWLtA7QQCQnCC4kNYQ0DtrBGcw+AMjUREAJCcIAJITBADJCQKA5AQBQHKC4EJuBQn0zvbRC9lKCvRORQCQnCC4gLYQMAJBcCYXngNGIQhWpKIAWmCx+ERz3qnMQjPQAhXBCXZn72bzwAgEwQXM6IERCIITbA/8QgAYhTWCEwkAYDQqAoDkBAFAcoIAIDlBAJCcIABIThAAJCcIAJITBADJCQKA5AQBQHKCACA5QQCQnCAASM7VRxsy593PAKZSETTK3c+Aa1ERXNmhWf+jmwfPHp9SERyrIFQYwBQqgiuaMst/dPPgrEHb/ZSBcwmCmUwd5Jeyr7q4xr8L9K/UWic/+cXyUn1YXl3wcABYwuv1tXinvlX2PaYiAEhOEAAkJwgAkrN99AKPnz5peiF2dwG75WMF1qMiOMPjp0+eDbKtbdPcPjYDPzCFXUNnanG23eIxAW2wa2gBre3Tb60yAfohCC5w7lnASzh0QhnAIRaLB2LwB84hCDrkYnLAnLSGOmMtAJibIABITmuoM1pBwNwEQSP0/YG1aA01QN8fWJMgWFnr1ysCxqc11ABhAKxJRbCyTQBsLhanTQRcmyBYmYEfWJsgAEhOEAAkJwhW5qqhwNrsGmqAwR9Yk4pgRXYJAS0QBCvZDoA1AkEIARuCACA5awSNmGudYDPLP/b9rEsAGyqCleze73iONs1uuwlgCkEAkJzW0EqWmLFnave4UB/MR0Wwgn0hYFCbTtsL5qUiGEyGO52N+rpgLSqCFWQYyMzaoR+CYCXbu4ZGC4ZNCDhpDfqgNbSyuUNgqVDJ0HKCrFQEHHXqrH47KIQGtE9FwFHbt9OcOrALAOiHioDJDO4wJkEAkJzWUMemXmBu39ec+nVzfT3QHhVBp9a4wJytoDAmFUEyl8ziH908uDgMzqligGUJggHsDqpLDraXfE8VBbRJEDTq2GB+aEBucbZtbQHaJQgatNv/H2HgHOE1wKgsFgMkJwgAktMaapA2CnBNKgKA5ATBgua+Hr/r+wNLEAQLWePMX4BzWCPoiLUDYAkqgoXcd7Zv63o5TmA+gmBB2/cknms2v+Q6wfa9hoE8BMHCrnEP4bloPUFOgqAzSw/Wc1YvQB8EAUBydg11yIwdmJOKACA5FUEi7gkA7CMIkhEAwC6toUSEALCPIOiQi88BcxIEHRMGwBwEQYe0eIA5WSzulDAA5qIiAEhOEAAkJwgAkhMEAMkJAoDkBAFAcoIAIDlBAJCcIABIThAAJCcIAJITBADJCQKA5AQBQHKCACA5QQCQnCAASE4QACQnCACSEwQAyQkCgOQEAUByggAgOUEAkJwgAEhOEAAkJwgAkhMEAMkJAoDkBAFAcoIAILlSa53+5FL+PSK+uNzhALCQl2utH973wElBAMB4tIYAkhMEAMkJAoDkBAFAcoIAIDlBAJCcIABIThAAJCcIAJL7f+xvG0RAkzbxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = rle_decode(submission.loc[0].predicted, shape=(25784, 34937))\n",
    "visualize_tensor(image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
